{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "World     : Grid \n",
    "\n",
    "Algorithm : N-step SARSA\n",
    "\n",
    "Sampling  : On-Policy\n",
    "\n",
    "Policy    : Softmax\n",
    "\n",
    "Paramterization : Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reward -1\n",
    "# we can move through potholes but we would take Reward -5 for time wasted\n",
    "# actions = (\"up\",\"right\",\"down\",\"left\") \n",
    "# state = (\"N\",\"NE\",\"E\",\"SE\",\"S\",\"SW\",\"W\",\"NW\", \"self_x\", \"self_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_board(state):\n",
    "    \n",
    "    print(state[7], state[0], state[1])\n",
    "    print(state[6], world[int(state[8])][int(state[9])], state[2])\n",
    "    print(state[5], state[4], state[3])\n",
    "\n",
    "    print(\"\\nPos :\",state[8],state[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_world(length,t_x,t_y,a_x,a_y):\n",
    "    choices = [1,1,0,0,0]\n",
    "\n",
    "    world = np.random.choice(choices, size=(length,length))\n",
    "\n",
    "    #agent position\n",
    "    #target position\n",
    "\n",
    "    world[t_x][t_y] = -1\n",
    "    world[a_x][a_y] = 0\n",
    "\n",
    "    return world\n",
    "# 1 are potholes walls, any cell outside the world is inaccesible an throws you back with -2 reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knows every cell around it, even diagonal ones, and its own position\n",
    "\n",
    "def env(action,state):\n",
    "    new_state = state.clone().detach()\n",
    "    \n",
    "    \n",
    "    a_x = int(new_state[8]*length -1)\n",
    "    a_y = int(new_state[9]*length -1)\n",
    "    # print(a_x,a_y)\n",
    "    reward = torch.tensor(-1)\n",
    "    \n",
    "    if action == \"u\":\n",
    "        if a_x == 0:\n",
    "            reward -= 4\n",
    "            e_t = state/2\n",
    "            e_t[8] = 0\n",
    "            e_t[9] = 0 \n",
    "            e_t += new_state\n",
    "            return reward, new_state,e_t, a_x,a_y\n",
    "        a_x += -1\n",
    "\n",
    "    elif action == \"d\":\n",
    "        if a_x == length-1:\n",
    "            reward -= 4\n",
    "            e_t = state/2\n",
    "            e_t[8] = 0\n",
    "            e_t[9] = 0 \n",
    "            e_t += new_state\n",
    "            return reward, new_state,e_t, a_x,a_y\n",
    "        a_x += 1\n",
    "    \n",
    "    elif action == \"r\":\n",
    "        if a_y == length-1:\n",
    "            reward -= 4\n",
    "            e_t = state/2\n",
    "            e_t[8] = 0\n",
    "            e_t[9] = 0 \n",
    "            e_t += new_state\n",
    "            return reward, new_state,e_t, a_x,a_y\n",
    "        a_y += 1\n",
    "    \n",
    "    elif action == \"l\":\n",
    "        if a_y == 0:\n",
    "            reward -= 4\n",
    "            e_t = state/2\n",
    "            e_t[8] = 0\n",
    "            e_t[9] = 0 \n",
    "            e_t += new_state\n",
    "            return reward, new_state,e_t, a_x,a_y\n",
    "        a_y += -1\n",
    "    \n",
    "    new_state[8] = (a_x+1)/length\n",
    "    new_state[9] = (a_y+1)/length\n",
    "    \n",
    "    if a_x>0:\n",
    "        new_state[0] = world[a_x-1][a_y]\n",
    "    else:\n",
    "        new_state[0] = 1\n",
    "\n",
    "    if a_x>0 and a_y<length-1:\n",
    "        new_state[1] = world[a_x-1][a_y+1]\n",
    "    else:\n",
    "        new_state[1] = 1\n",
    "\n",
    "    if a_y<length-1:\n",
    "        new_state[2] = world[a_x][a_y+1]\n",
    "    else:\n",
    "        new_state[2] = 1\n",
    "    \n",
    "    if a_x<length-1 and a_y<length-1:\n",
    "        new_state[3] = world[a_x+1][a_y+1]\n",
    "    else:\n",
    "        new_state[3] = 1\n",
    "    \n",
    "    if a_x<length-1:\n",
    "        new_state[4] = world[a_x+1][a_y]\n",
    "    else:\n",
    "        new_state[4] = 1\n",
    "\n",
    "    if a_x<length-1 and a_y>0:\n",
    "        new_state[5] = world[a_x+1][a_y-1]\n",
    "    else:\n",
    "        new_state[5] = 1\n",
    "\n",
    "    if a_y>0:    \n",
    "        new_state[6] = world[a_x][a_y-1]\n",
    "    else:\n",
    "        new_state[6] = 1\n",
    "\n",
    "    if a_x>0 and a_y>0:\n",
    "        new_state[7] = world[a_x-1][a_y-1]\n",
    "    else:\n",
    "        new_state[7] = 1\n",
    "\n",
    "\n",
    "    if world[a_x][a_y] == 1:\n",
    "        reward -= 4\n",
    "\n",
    "    elif world[a_x][a_y]== -1:\n",
    "        reward += 10\n",
    "    \n",
    "    \n",
    "    e_t = state/2\n",
    "    e_t[8] = 0\n",
    "    e_t[9] = 0 \n",
    "    e_t += new_state\n",
    "\n",
    "    return reward, new_state,e_t, a_x,a_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple 1 hidden layer NN\n",
    "\n",
    "class FApproxModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(10, 40)\n",
    "        self.linear2 = torch.nn.Linear(40, 20)\n",
    "        self.linear3 = torch.nn.Linear(20, 10)\n",
    "        self.linear4 = torch.nn.Linear(10, 4)\n",
    "    \n",
    "    def forward(self,features):\n",
    "        \n",
    "        out = torch.relu(self.linear1(features))\n",
    "        out = torch.relu(self.linear2(out))\n",
    "        out = self.linear3(out)        \n",
    "        action_pref = self.linear4(out)\n",
    "        \n",
    "        return action_pref\n",
    "\n",
    "Q = FApproxModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_policy(action_pref, actions=(\"u\",\"r\",\"d\",\"l\") ):\n",
    "\n",
    "    # print(action_pref)\n",
    "    action_prob = torch.softmax(torch.relu(action_pref), -1)\n",
    "    # print(action_prob)\n",
    "    \n",
    "    choice = random.choices((0,1,2,3),action_prob)\n",
    "    \n",
    "    return actions[choice[0]],action_pref[choice[0]],action_prob[choice[0]]\n",
    "\n",
    "\n",
    "def e_greedy_policy(h_f,a_x,a_y, actions=(\"u\",\"r\",\"d\",\"l\")):\n",
    "    greedy = random.choices((0,1),(0.30,0.70))[0]\n",
    "    \n",
    "    if greedy:\n",
    "        choice = torch.where(h_f==h_f.max())[0][0]\n",
    "        \n",
    "    else:\n",
    "        if a_x<15 and a_y<15:\n",
    "            if a_x<a_y:\n",
    "                choice = 2\n",
    "            else: \n",
    "                choice = 1\n",
    "        else:\n",
    "            return softmax_policy(h_f)\n",
    "            \n",
    "    return actions[choice],h_f[choice],0\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(state_temp,e_t,n,r,gamma=0.9):\n",
    "    q = int(r)\n",
    "    discount = gamma\n",
    "    \n",
    "    for i in range(1,n):\n",
    "        action_pref = Q(e_t)\n",
    "        a,q_T,p_t = e_greedy_policy(action_pref,a_x,a_y)\n",
    "\n",
    "        reward, state_temp,e_t,a_x,a_y = env(a, state_temp)\n",
    "        \n",
    "        q += discount*reward\n",
    "        discount *= gamma\n",
    "\n",
    "    action_pref = Q(state_temp)\n",
    "    with torch.no_grad():\n",
    "        q += discount*action_pref.max()\n",
    "\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn(Q_pi,Q_t):\n",
    "\n",
    "#     error = Q_pi - Q_t\n",
    "#     loss = 0.5*(error**2)\n",
    "\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(Q.parameters(), lr=0.0001)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_episode = 0\n",
    "history = []\n",
    "\n",
    "length = 20\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100\n",
    "\n",
    "\n",
    "for episode in range(episodes):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        net_episode+=1\n",
    "\n",
    "        G = torch.tensor(0)\n",
    "        a_x, a_y = 0,0\n",
    "        t_x, t_y = np.random.randint(length/2+length/5,length), np.random.randint(length/2+length/5,length)\n",
    "        world = torch.tensor(gen_world(20, t_x,t_y, a_x,a_y),dtype=torch.int8)\n",
    "        path = torch.zeros((length,length),dtype=torch.int8)\n",
    "        state = torch.Tensor([1,1,world[0][1],world[1][1],world[1][0],1,1,1,(a_x+1)/length,(a_y+1)/length])\n",
    "\n",
    "        path[a_x][a_y] = 1\n",
    "        t = 1\n",
    "    opt.zero_grad()\n",
    "    while world[a_x][a_y] != -1 and t<5000:\n",
    "        \n",
    "        h_f = Q(state)\n",
    "        a,q_t,p_t = e_greedy_policy(h_f,a_x,a_y)\n",
    "        # print(action_pref)\n",
    "        # print(a)\n",
    "        \n",
    "        reward, state,e_t, a_x,a_y = env(a, state)\n",
    "\n",
    "        # n-step SARSA \n",
    "        # q_pi = sample(state,e_t, 5,reward, a_x,a_y)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            max_q = h_f.max()\n",
    "        q_pi = reward + 0.9*max_q\n",
    "\n",
    "        loss = loss_fn(q_pi, q_t)\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "        opt.zero_grad()  \n",
    "\n",
    "        with torch.no_grad():\n",
    "            G += reward\n",
    "\n",
    "            if path[a_x][a_y] + 1 <10:\n",
    "                path[a_x][a_y] += 1\n",
    "            \n",
    "            t += 1\n",
    "            # if t%1000 == 0:\n",
    "            #     print(\".\",end=\"\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        print(\"Episode :\", net_episode, \" Score :\", int(G), \" Time :\",t)\n",
    "\n",
    "        if G>-90:\n",
    "            print(G, t, G/t)\n",
    "            print(path)\n",
    "            \n",
    "            torch.save(Q.state_dict(),f\"{G}|{t}.pth\")\n",
    "\n",
    "        # print(G/t)\n",
    "        # print(path)\n",
    "\n",
    "        # print(\"Distance\",d,\"%\")\n",
    "        # history.append((G,t,G/t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
